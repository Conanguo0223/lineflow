{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f210e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import ast\n",
    "import argparse\n",
    "import os\n",
    "from lineflow.helpers import get_device\n",
    "from lineflow.learning.helpers import (\n",
    "    make_stacked_vec_env,\n",
    ")\n",
    "from lineflow.learning.curriculum import CurriculumLearningCallback\n",
    "from lineflow.examples import (\n",
    "    WaitingTime,\n",
    "    ComplexLine\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import (\n",
    "    HeteroConv,\n",
    "    GCNConv,\n",
    "    SAGEConv,\n",
    "    TransformerConv,\n",
    "    HGTConv,\n",
    ")\n",
    "from torch.nn import Linear\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "22f1a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('../data/complex_line_graph_n_assemblies5_waiting_time5.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a58fe9d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'snapshot_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric_temporal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m temporal_signal_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# loader = ChickenpoxDatasetLoader()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dataset = loader.get_dataset()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_signal_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch_geometric_temporal/signal/train_test_split.py:49\u001b[0m, in \u001b[0;36mtemporal_signal_split\u001b[0;34m(data_iterator, train_ratio)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtemporal_signal_split\u001b[39m(\n\u001b[1;32m     37\u001b[0m     data_iterator, train_ratio: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Discrete_Signal, Discrete_Signal]:\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Function to split a data iterator according to a fixed ratio.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Arg types:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m        * **(train_iterator, test_iterator)** *(tuple of Signal Iterators)* - Train and test data iterators.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     train_snapshots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_ratio \u001b[38;5;241m*\u001b[39m \u001b[43mdata_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnapshot_count\u001b[49m)\n\u001b[1;32m     51\u001b[0m     train_iterator \u001b[38;5;241m=\u001b[39m data_iterator[\u001b[38;5;241m0\u001b[39m:train_snapshots]\n\u001b[1;32m     52\u001b[0m     test_iterator \u001b[38;5;241m=\u001b[39m data_iterator[train_snapshots:]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'snapshot_count'"
     ]
    }
   ],
   "source": [
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "# loader = ChickenpoxDatasetLoader()\n",
    "\n",
    "# dataset = loader.get_dataset()\n",
    "\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "27a9d103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ddbcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hetero_to_networkx(data):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for node_type in data.node_types:\n",
    "        for i in range(data[node_type].num_nodes):\n",
    "            G.add_node((node_type, i), node_type=node_type)\n",
    "\n",
    "    # Add edges with types\n",
    "    for edge_type in data.edge_types:\n",
    "        src_type, rel_type, dst_type = edge_type\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        for src, dst in zip(edge_index[0], edge_index[1]):\n",
    "            G.add_edge((src_type, int(src)), (dst_type, int(dst)), key=rel_type, rel_type=rel_type)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4feb172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manufacturing_layout(G):\n",
    "    \"\"\"Create a hierarchical layout for manufacturing systems\"\"\"\n",
    "    pos = {}\n",
    "    \n",
    "    # Define hierarchy levels\n",
    "    levels = {\n",
    "        'Source': 0,\n",
    "        'Magazine': 1, \n",
    "        'Assembly': 2,\n",
    "        'Switch': 3,\n",
    "        'Sink': 4,\n",
    "        'WorkerPool': 1.5,  # Between magazine and assembly\n",
    "        'Worker': 1.5\n",
    "    }\n",
    "    \n",
    "    # Group nodes by type\n",
    "    node_groups = {}\n",
    "    for node in G.nodes():\n",
    "        node_type = node[0]\n",
    "        if node_type not in node_groups:\n",
    "            node_groups[node_type] = []\n",
    "        node_groups[node_type].append(node)\n",
    "    \n",
    "    # Position nodes by level and spread horizontally\n",
    "    for node_type, nodes in node_groups.items():\n",
    "        level = levels.get(node_type, 2)\n",
    "        for i, node in enumerate(nodes):\n",
    "            x = i - len(nodes)/2  # Center horizontally\n",
    "            y = -level  # Negative to flow top to bottom\n",
    "            pos[node] = (x, y)\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = ComplexLine(use_graph_as_states=True, n_workers = 3)\n",
    "# data = line._graph_states\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import OGB_MAG\n",
    "\n",
    "# dataset = OGB_MAG(root='./data', preprocess='metapath2vec')\n",
    "# data = dataset[0]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# data = data['graph'][0]\n",
    "G = hetero_to_networkx(data)\n",
    "\n",
    "# Layout\n",
    "# pos = nx.spring_layout(G, seed=42)\n",
    "pos = create_manufacturing_layout(G) \n",
    "pos = nx.shell_layout(G)  \n",
    "# Draw nodes with type labels\n",
    "node_colors = {\n",
    "    'Assembly': 'skyblue',\n",
    "    'Sink': 'lightgreen',\n",
    "    'Source': 'salmon',\n",
    "    'Switch': 'lightcoral',\n",
    "    'WorkerPool': 'lightblue',\n",
    "    'Magazine': 'lightgrey',\n",
    "    'Worker': 'yellow',\n",
    "}\n",
    "for node_type in data.node_types:\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        nodelist=[n for n in G.nodes if n[0] == node_type],\n",
    "        node_color=node_colors[node_type],\n",
    "        label=node_type,\n",
    "        node_size=500\n",
    "    )\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, arrows=True)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, labels={n: f\"{n[0][0]}{n[1]}\" for n in G.nodes}, font_size=10)\n",
    "\n",
    "# Legend\n",
    "plt.legend(handles=[\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Assembly', markerfacecolor='skyblue', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Sink', markerfacecolor='lightgreen', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Source', markerfacecolor='salmon', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Switch', markerfacecolor='lightcoral', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='WorkerPool', markerfacecolor='lightblue', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Magazine', markerfacecolor='lightgrey', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Worker', markerfacecolor='yellow', markersize=10)\n",
    "])\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a0f1abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GCNConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv,TransformerConv, HGTConv\n",
    "\n",
    "\n",
    "class GraphStatePredictor(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, data, context_window=5):\n",
    "        super().__init__()\n",
    "        self.context_window = context_window\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.data = data\n",
    "        # Linear layers for each node type to project to hidden dimension\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            in_channels = data.x_dict[node_type].shape[1]\n",
    "            self.lin_dict[node_type] = Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # HGT layers for processing individual graphs\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(), num_heads)\n",
    "            self.graph_convs.append(conv)\n",
    "        \n",
    "        # Temporal transformer to process sequence of graph states\n",
    "        self.temporal_transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_channels,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_channels * 4,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Output projection for each node type\n",
    "        self.output_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            out_features = data.x_dict[node_type].shape[1]\n",
    "            self.output_dict[node_type] = Linear(hidden_channels, out_features)\n",
    "    \n",
    "    def encode_single_graph(self, x_dict, edge_index_dict):\n",
    "        # Project node features to hidden dimension\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Apply HGT convolutions\n",
    "        for conv in self.graph_convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "        \n",
    "        return x_dict\n",
    "    \n",
    "    def forward(self, graph_sequence):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            graph_sequence: List of HeteroData objects representing graph states\n",
    "        Returns:\n",
    "            Predicted next graph state as dict of node type tensors\n",
    "        \"\"\"\n",
    "        batch_size = len(graph_sequence)\n",
    "        \n",
    "        # Encode each graph in the sequence\n",
    "        encoded_sequence = {}\n",
    "        for node_type in self.data.node_types:\n",
    "            encoded_sequence[node_type] = []\n",
    "        \n",
    "        for graph in graph_sequence:\n",
    "            encoded_graph = self.encode_single_graph(graph.x_dict, graph.edge_index_dict)\n",
    "            for node_type in self.data.node_types:\n",
    "                encoded_sequence[node_type].append(encoded_graph[node_type])\n",
    "        \n",
    "        # Apply temporal transformer for each node type\n",
    "        predictions = {}\n",
    "        for node_type in self.data.node_types:\n",
    "            # Stack temporal sequence: [seq_len, num_nodes, hidden_dim]\n",
    "            node_sequence = torch.stack(encoded_sequence[node_type], dim=0)\n",
    "            num_nodes = node_sequence.shape[1]\n",
    "            \n",
    "            # Reshape for transformer: [num_nodes, seq_len, hidden_dim]\n",
    "            node_sequence = node_sequence.transpose(0, 1)\n",
    "            \n",
    "            # Apply temporal transformer to each node independently\n",
    "            temporal_output = []\n",
    "            for node_idx in range(num_nodes):\n",
    "                node_temporal = node_sequence[node_idx].unsqueeze(0)  # [1, seq_len, hidden_dim]\n",
    "                transformed = self.temporal_transformer(node_temporal)\n",
    "                temporal_output.append(transformed[:, -1, :])  # Take last timestep\n",
    "            \n",
    "            temporal_features = torch.cat(temporal_output, dim=0)  # [num_nodes, hidden_dim]\n",
    "            \n",
    "            # Project to output dimension\n",
    "            predictions[node_type] = self.output_dict[node_type](temporal_features)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "\n",
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            in_channels = data.x_dict[node_type].shape[1]\n",
    "            self.lin_dict[node_type] = Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           num_heads)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        return x_dict\n",
    "\n",
    "\n",
    "# model = HGT(hidden_channels=64, out_channels=4, num_heads=2, num_layers=3)\n",
    "\n",
    "# # model = HGT(state.metadata())  # metadata = (node_types, edge_types)\n",
    "# out = model(data.x_dict, data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "21ae0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('../data/complex_line_graph_n_assemblies5_waiting_time5.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "574b281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.8"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset['Reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0db874b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total datasets loaded: 28\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# # Get all .pt files in the data folder\n",
    "# data_files = glob.glob('../data/*.pt')\n",
    "\n",
    "# # Initialize list to store all datasets\n",
    "# all_data_sets = []\n",
    "\n",
    "# # Loop through each .pt file and load it\n",
    "# for file_path in data_files:\n",
    "#     try:\n",
    "#         dataset = torch.load(file_path, weights_only=False)\n",
    "#         all_data_sets.append({\n",
    "#             'filename': os.path.basename(file_path),\n",
    "#             'data': dataset\n",
    "#         })\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# print(f\"\\nTotal datasets loaded: {len(all_data_sets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "346e06d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 6]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_indices = 3\n",
    "random_indices = random.sample(range(len(all_data_sets)), num_indices)\n",
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5bd1f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Average Loss: 291.9907, Max Loss: 396.0925\n",
      "Epoch 2/1000, Average Loss: 206.7878, Max Loss: 329.6706\n",
      "Epoch 4/1000, Average Loss: 161.4691, Max Loss: 283.4834\n",
      "Epoch 6/1000, Average Loss: 112.6697, Max Loss: 246.9769\n",
      "Epoch 8/1000, Average Loss: 109.1962, Max Loss: 229.1213\n",
      "Epoch 10/1000, Average Loss: 89.8363, Max Loss: 200.8033\n",
      "Epoch 12/1000, Average Loss: 80.8441, Max Loss: 121.5740\n",
      "Epoch 14/1000, Average Loss: 52.0669, Max Loss: 144.9328\n",
      "Epoch 16/1000, Average Loss: 34.3432, Max Loss: 123.7318\n",
      "Epoch 18/1000, Average Loss: 36.3181, Max Loss: 95.4925\n",
      "Epoch 20/1000, Average Loss: 29.3887, Max Loss: 92.6286\n",
      "Epoch 22/1000, Average Loss: 32.3525, Max Loss: 73.4036\n",
      "Epoch 24/1000, Average Loss: 32.4819, Max Loss: 85.1250\n",
      "Epoch 26/1000, Average Loss: 47.5109, Max Loss: 404.3418\n",
      "Epoch 28/1000, Average Loss: 22.9424, Max Loss: 73.1668\n",
      "Epoch 30/1000, Average Loss: 25.5181, Max Loss: 50.7978\n",
      "Epoch 32/1000, Average Loss: 22.2496, Max Loss: 55.0084\n",
      "Epoch 34/1000, Average Loss: 20.1637, Max Loss: 69.8338\n",
      "Epoch 36/1000, Average Loss: 13.8328, Max Loss: 28.5314\n",
      "Epoch 38/1000, Average Loss: 9.2538, Max Loss: 33.2491\n",
      "Epoch 40/1000, Average Loss: 19.2933, Max Loss: 95.0109\n",
      "Epoch 42/1000, Average Loss: 18.9981, Max Loss: 92.8369\n",
      "Epoch 44/1000, Average Loss: 14.3759, Max Loss: 74.2375\n",
      "Epoch 46/1000, Average Loss: 12.3182, Max Loss: 48.1016\n",
      "Epoch 48/1000, Average Loss: 16.3583, Max Loss: 83.2510\n",
      "Epoch 50/1000, Average Loss: 11.8921, Max Loss: 27.3703\n",
      "Epoch 52/1000, Average Loss: 14.2308, Max Loss: 39.6508\n",
      "Epoch 54/1000, Average Loss: 12.8989, Max Loss: 32.4545\n",
      "Epoch 56/1000, Average Loss: 15.4228, Max Loss: 105.4214\n",
      "Epoch 58/1000, Average Loss: 21.8414, Max Loss: 100.5369\n",
      "Epoch 60/1000, Average Loss: 15.5245, Max Loss: 56.2246\n",
      "Epoch 62/1000, Average Loss: 21.6039, Max Loss: 63.0803\n",
      "Epoch 64/1000, Average Loss: 16.0057, Max Loss: 62.1820\n",
      "Epoch 66/1000, Average Loss: 22.4918, Max Loss: 127.5280\n",
      "Epoch 68/1000, Average Loss: 9.2453, Max Loss: 43.5026\n",
      "Epoch 70/1000, Average Loss: 37.5848, Max Loss: 361.9800\n",
      "Epoch 72/1000, Average Loss: 12.5172, Max Loss: 75.4406\n",
      "Epoch 74/1000, Average Loss: 14.3158, Max Loss: 93.6967\n",
      "Epoch 76/1000, Average Loss: 4.8658, Max Loss: 9.8294\n",
      "Epoch 78/1000, Average Loss: 11.4981, Max Loss: 104.4334\n",
      "Epoch 80/1000, Average Loss: 15.2950, Max Loss: 61.4756\n",
      "Epoch 82/1000, Average Loss: 11.6331, Max Loss: 93.8519\n",
      "Epoch 84/1000, Average Loss: 10.0269, Max Loss: 85.0589\n",
      "Epoch 86/1000, Average Loss: 9.5614, Max Loss: 49.8178\n",
      "Epoch 88/1000, Average Loss: 14.9855, Max Loss: 96.5331\n",
      "Epoch 90/1000, Average Loss: 11.5399, Max Loss: 149.1919\n",
      "Epoch 92/1000, Average Loss: 17.8604, Max Loss: 109.0188\n",
      "Epoch 94/1000, Average Loss: 3.6392, Max Loss: 5.3163\n",
      "Epoch 96/1000, Average Loss: 17.7288, Max Loss: 95.0415\n",
      "Epoch 98/1000, Average Loss: 43.0529, Max Loss: 366.1212\n",
      "Epoch 100/1000, Average Loss: 7.1236, Max Loss: 21.0397\n",
      "Epoch 102/1000, Average Loss: 8.6976, Max Loss: 90.2901\n",
      "Epoch 104/1000, Average Loss: 11.3662, Max Loss: 85.4661\n",
      "Epoch 106/1000, Average Loss: 19.1654, Max Loss: 73.9118\n",
      "Epoch 108/1000, Average Loss: 7.6177, Max Loss: 21.6705\n",
      "Epoch 110/1000, Average Loss: 10.7582, Max Loss: 77.0132\n",
      "Epoch 112/1000, Average Loss: 14.1547, Max Loss: 76.4600\n",
      "Epoch 114/1000, Average Loss: 6.4600, Max Loss: 52.9658\n",
      "Epoch 116/1000, Average Loss: 4.7371, Max Loss: 7.8172\n",
      "Epoch 118/1000, Average Loss: 12.4288, Max Loss: 76.7912\n",
      "Epoch 120/1000, Average Loss: 7.4640, Max Loss: 36.4916\n",
      "Epoch 122/1000, Average Loss: 4.7444, Max Loss: 16.1340\n",
      "Epoch 124/1000, Average Loss: 4.7574, Max Loss: 10.7908\n",
      "Epoch 126/1000, Average Loss: 7.9614, Max Loss: 103.4293\n",
      "Epoch 128/1000, Average Loss: 15.2504, Max Loss: 76.3699\n",
      "Epoch 130/1000, Average Loss: 32.7028, Max Loss: 366.5705\n",
      "Epoch 132/1000, Average Loss: 8.6482, Max Loss: 45.6500\n",
      "Epoch 134/1000, Average Loss: 11.1809, Max Loss: 101.4001\n",
      "Epoch 136/1000, Average Loss: 13.5444, Max Loss: 103.9590\n",
      "Epoch 138/1000, Average Loss: 14.1968, Max Loss: 83.5175\n",
      "Epoch 140/1000, Average Loss: 11.8983, Max Loss: 69.7711\n",
      "Epoch 142/1000, Average Loss: 10.9122, Max Loss: 39.3656\n",
      "Epoch 144/1000, Average Loss: 10.8380, Max Loss: 70.0226\n",
      "Epoch 146/1000, Average Loss: 12.5084, Max Loss: 81.5700\n",
      "Epoch 148/1000, Average Loss: 8.0278, Max Loss: 62.6320\n",
      "Epoch 150/1000, Average Loss: 8.7463, Max Loss: 53.7941\n",
      "Epoch 152/1000, Average Loss: 8.2704, Max Loss: 87.9729\n",
      "Epoch 154/1000, Average Loss: 10.5111, Max Loss: 88.4583\n",
      "Epoch 156/1000, Average Loss: 31.5236, Max Loss: 135.9581\n",
      "Epoch 158/1000, Average Loss: 20.2798, Max Loss: 124.1841\n",
      "Epoch 160/1000, Average Loss: 16.2465, Max Loss: 84.1394\n",
      "Epoch 162/1000, Average Loss: 10.9497, Max Loss: 82.1969\n",
      "Epoch 164/1000, Average Loss: 7.5358, Max Loss: 38.8160\n",
      "Epoch 166/1000, Average Loss: 10.3375, Max Loss: 97.8972\n",
      "Epoch 168/1000, Average Loss: 17.3480, Max Loss: 78.1820\n",
      "Epoch 170/1000, Average Loss: 8.0217, Max Loss: 69.3726\n",
      "Epoch 172/1000, Average Loss: 4.2318, Max Loss: 11.4687\n",
      "Epoch 174/1000, Average Loss: 12.1025, Max Loss: 72.9881\n",
      "Epoch 176/1000, Average Loss: 5.0899, Max Loss: 33.2380\n",
      "Epoch 178/1000, Average Loss: 20.9795, Max Loss: 95.0145\n",
      "Epoch 180/1000, Average Loss: 8.0796, Max Loss: 75.3893\n",
      "Epoch 182/1000, Average Loss: 10.0533, Max Loss: 69.7549\n",
      "Epoch 184/1000, Average Loss: 8.8931, Max Loss: 77.1943\n",
      "Epoch 186/1000, Average Loss: 6.5722, Max Loss: 50.6159\n",
      "Epoch 188/1000, Average Loss: 48.7258, Max Loss: 366.7267\n",
      "Epoch 190/1000, Average Loss: 4.2815, Max Loss: 9.9634\n",
      "Epoch 192/1000, Average Loss: 8.1022, Max Loss: 82.1417\n",
      "Epoch 194/1000, Average Loss: 4.8663, Max Loss: 10.9606\n",
      "Epoch 196/1000, Average Loss: 20.7902, Max Loss: 93.4700\n",
      "Epoch 198/1000, Average Loss: 12.6031, Max Loss: 72.5823\n",
      "Epoch 200/1000, Average Loss: 6.9353, Max Loss: 46.7647\n",
      "Epoch 202/1000, Average Loss: 16.2559, Max Loss: 86.0347\n",
      "Epoch 204/1000, Average Loss: 10.4502, Max Loss: 79.9891\n",
      "Epoch 206/1000, Average Loss: 17.4033, Max Loss: 84.8950\n",
      "Epoch 208/1000, Average Loss: 5.7432, Max Loss: 63.7746\n",
      "Epoch 210/1000, Average Loss: 4.2187, Max Loss: 16.0509\n",
      "Epoch 212/1000, Average Loss: 10.6840, Max Loss: 94.1824\n",
      "Epoch 214/1000, Average Loss: 8.9242, Max Loss: 70.9418\n",
      "Epoch 216/1000, Average Loss: 3.5985, Max Loss: 6.8031\n",
      "Epoch 218/1000, Average Loss: 18.3487, Max Loss: 145.5347\n",
      "Epoch 220/1000, Average Loss: 7.1325, Max Loss: 76.3807\n",
      "Epoch 222/1000, Average Loss: 5.3273, Max Loss: 31.9774\n",
      "Epoch 224/1000, Average Loss: 13.8835, Max Loss: 81.9785\n",
      "Epoch 226/1000, Average Loss: 10.5270, Max Loss: 109.8344\n",
      "Epoch 228/1000, Average Loss: 36.4610, Max Loss: 366.7166\n",
      "Epoch 230/1000, Average Loss: 10.8164, Max Loss: 68.8478\n",
      "Epoch 232/1000, Average Loss: 9.8446, Max Loss: 80.2328\n",
      "Epoch 234/1000, Average Loss: 6.1480, Max Loss: 64.7367\n",
      "Epoch 236/1000, Average Loss: 12.6515, Max Loss: 59.2547\n",
      "Epoch 238/1000, Average Loss: 9.8561, Max Loss: 85.3462\n",
      "Epoch 240/1000, Average Loss: 17.9032, Max Loss: 135.0517\n",
      "Epoch 242/1000, Average Loss: 4.2713, Max Loss: 23.0545\n",
      "Epoch 244/1000, Average Loss: 10.3675, Max Loss: 68.2304\n",
      "Epoch 246/1000, Average Loss: 10.7124, Max Loss: 56.0686\n",
      "Epoch 248/1000, Average Loss: 13.8212, Max Loss: 90.9236\n",
      "Epoch 250/1000, Average Loss: 17.3691, Max Loss: 90.5664\n",
      "Epoch 252/1000, Average Loss: 16.6747, Max Loss: 58.2447\n",
      "Epoch 254/1000, Average Loss: 4.8349, Max Loss: 16.9489\n",
      "Epoch 256/1000, Average Loss: 2.9631, Max Loss: 4.3432\n",
      "Epoch 258/1000, Average Loss: 10.7156, Max Loss: 120.7022\n",
      "Epoch 260/1000, Average Loss: 7.6912, Max Loss: 49.6926\n",
      "Epoch 262/1000, Average Loss: 21.6618, Max Loss: 112.1935\n",
      "Epoch 264/1000, Average Loss: 15.7877, Max Loss: 97.4191\n",
      "Epoch 266/1000, Average Loss: 8.4926, Max Loss: 33.8006\n",
      "Epoch 268/1000, Average Loss: 11.0589, Max Loss: 65.7383\n",
      "Epoch 270/1000, Average Loss: 18.5548, Max Loss: 154.4552\n",
      "Epoch 272/1000, Average Loss: 11.8774, Max Loss: 94.2316\n",
      "Epoch 274/1000, Average Loss: 5.7740, Max Loss: 31.4524\n",
      "Epoch 276/1000, Average Loss: 19.9577, Max Loss: 161.1603\n",
      "Epoch 278/1000, Average Loss: 18.3926, Max Loss: 157.8454\n",
      "Epoch 280/1000, Average Loss: 9.8299, Max Loss: 50.2569\n",
      "Epoch 282/1000, Average Loss: 8.6050, Max Loss: 107.6568\n",
      "Epoch 284/1000, Average Loss: 10.3718, Max Loss: 52.1648\n",
      "Epoch 286/1000, Average Loss: 17.2939, Max Loss: 107.3954\n",
      "Epoch 288/1000, Average Loss: 12.4871, Max Loss: 90.6399\n",
      "Epoch 290/1000, Average Loss: 12.8506, Max Loss: 54.4554\n",
      "Epoch 292/1000, Average Loss: 9.9597, Max Loss: 89.3735\n",
      "Epoch 294/1000, Average Loss: 4.1253, Max Loss: 13.8055\n",
      "Epoch 296/1000, Average Loss: 18.5931, Max Loss: 149.1785\n",
      "Epoch 298/1000, Average Loss: 9.2881, Max Loss: 53.9947\n",
      "Epoch 300/1000, Average Loss: 7.4724, Max Loss: 51.6279\n",
      "Epoch 302/1000, Average Loss: 6.1600, Max Loss: 52.4328\n",
      "Epoch 304/1000, Average Loss: 6.2826, Max Loss: 32.6704\n",
      "Epoch 306/1000, Average Loss: 9.5151, Max Loss: 94.8884\n",
      "Epoch 308/1000, Average Loss: 5.1377, Max Loss: 31.3810\n",
      "Epoch 310/1000, Average Loss: 8.7941, Max Loss: 113.3372\n",
      "Epoch 312/1000, Average Loss: 14.4283, Max Loss: 89.6996\n",
      "Epoch 314/1000, Average Loss: 12.9910, Max Loss: 118.8867\n",
      "Epoch 316/1000, Average Loss: 7.4533, Max Loss: 83.1591\n",
      "Epoch 318/1000, Average Loss: 14.9742, Max Loss: 54.2815\n",
      "Epoch 320/1000, Average Loss: 3.9207, Max Loss: 8.9313\n",
      "Epoch 322/1000, Average Loss: 4.2506, Max Loss: 20.1881\n",
      "Epoch 324/1000, Average Loss: 7.3679, Max Loss: 35.9632\n",
      "Epoch 326/1000, Average Loss: 15.3843, Max Loss: 94.0864\n",
      "Epoch 328/1000, Average Loss: 14.7562, Max Loss: 111.8206\n",
      "Epoch 330/1000, Average Loss: 5.4925, Max Loss: 32.9832\n",
      "Epoch 332/1000, Average Loss: 5.9964, Max Loss: 57.1903\n",
      "Epoch 334/1000, Average Loss: 10.6821, Max Loss: 112.7115\n",
      "Epoch 336/1000, Average Loss: 9.6648, Max Loss: 49.4333\n",
      "Epoch 338/1000, Average Loss: 7.8780, Max Loss: 46.4278\n",
      "Epoch 340/1000, Average Loss: 6.7831, Max Loss: 41.6968\n",
      "Epoch 342/1000, Average Loss: 11.7294, Max Loss: 91.4575\n",
      "Epoch 344/1000, Average Loss: 7.0460, Max Loss: 62.1039\n",
      "Epoch 346/1000, Average Loss: 11.8310, Max Loss: 87.8525\n",
      "Epoch 348/1000, Average Loss: 10.8620, Max Loss: 95.2726\n",
      "Epoch 350/1000, Average Loss: 11.3921, Max Loss: 117.7433\n",
      "Epoch 352/1000, Average Loss: 15.7436, Max Loss: 106.6074\n",
      "Epoch 354/1000, Average Loss: 18.2264, Max Loss: 151.6883\n",
      "Epoch 356/1000, Average Loss: 8.4631, Max Loss: 102.2996\n",
      "Epoch 358/1000, Average Loss: 3.6395, Max Loss: 9.5771\n",
      "Epoch 360/1000, Average Loss: 11.0526, Max Loss: 90.0327\n",
      "Epoch 362/1000, Average Loss: 37.4380, Max Loss: 365.8157\n",
      "Epoch 364/1000, Average Loss: 3.0550, Max Loss: 13.2982\n",
      "Epoch 366/1000, Average Loss: 8.2165, Max Loss: 90.7934\n",
      "Epoch 368/1000, Average Loss: 10.2055, Max Loss: 46.6742\n",
      "Epoch 370/1000, Average Loss: 9.8132, Max Loss: 56.3649\n",
      "Epoch 372/1000, Average Loss: 7.7728, Max Loss: 29.6498\n",
      "Epoch 374/1000, Average Loss: 15.5522, Max Loss: 76.7062\n",
      "Epoch 376/1000, Average Loss: 9.1777, Max Loss: 90.9159\n",
      "Epoch 378/1000, Average Loss: 14.7750, Max Loss: 143.0917\n",
      "Epoch 380/1000, Average Loss: 6.6394, Max Loss: 53.3462\n",
      "Epoch 382/1000, Average Loss: 11.3310, Max Loss: 88.8997\n",
      "Epoch 384/1000, Average Loss: 9.1228, Max Loss: 77.4378\n",
      "Epoch 386/1000, Average Loss: 3.6825, Max Loss: 14.8289\n",
      "Epoch 388/1000, Average Loss: 3.3242, Max Loss: 9.6563\n",
      "Epoch 390/1000, Average Loss: 11.8378, Max Loss: 77.5155\n",
      "Epoch 392/1000, Average Loss: 16.3197, Max Loss: 103.2299\n",
      "Epoch 394/1000, Average Loss: 5.6497, Max Loss: 11.6507\n",
      "Epoch 396/1000, Average Loss: 15.8377, Max Loss: 140.6233\n",
      "Epoch 398/1000, Average Loss: 11.7302, Max Loss: 66.2995\n",
      "Epoch 400/1000, Average Loss: 10.3670, Max Loss: 76.4861\n",
      "Epoch 402/1000, Average Loss: 12.5129, Max Loss: 96.7768\n",
      "Epoch 404/1000, Average Loss: 4.2238, Max Loss: 9.6360\n",
      "Epoch 406/1000, Average Loss: 15.2290, Max Loss: 96.8708\n",
      "Epoch 408/1000, Average Loss: 10.0360, Max Loss: 48.4009\n",
      "Epoch 410/1000, Average Loss: 16.1455, Max Loss: 90.2459\n",
      "Epoch 412/1000, Average Loss: 4.7605, Max Loss: 15.6529\n",
      "Epoch 414/1000, Average Loss: 10.5068, Max Loss: 71.8902\n",
      "Epoch 416/1000, Average Loss: 8.4156, Max Loss: 101.8332\n",
      "Epoch 418/1000, Average Loss: 4.4627, Max Loss: 23.6483\n",
      "Epoch 420/1000, Average Loss: 2.7743, Max Loss: 5.1103\n",
      "Epoch 422/1000, Average Loss: 4.5445, Max Loss: 31.6523\n",
      "Epoch 424/1000, Average Loss: 8.8622, Max Loss: 76.4322\n",
      "Epoch 426/1000, Average Loss: 9.6758, Max Loss: 62.7852\n",
      "Epoch 428/1000, Average Loss: 2.3430, Max Loss: 5.3670\n",
      "Epoch 430/1000, Average Loss: 12.6889, Max Loss: 128.3285\n",
      "Epoch 432/1000, Average Loss: 14.3766, Max Loss: 98.4421\n",
      "Epoch 434/1000, Average Loss: 4.8985, Max Loss: 28.9155\n",
      "Epoch 436/1000, Average Loss: 15.5813, Max Loss: 64.8641\n",
      "Epoch 438/1000, Average Loss: 8.2104, Max Loss: 106.6612\n",
      "Epoch 440/1000, Average Loss: 14.7204, Max Loss: 158.0532\n",
      "Epoch 442/1000, Average Loss: 12.5318, Max Loss: 43.9686\n",
      "Epoch 444/1000, Average Loss: 6.5546, Max Loss: 28.5387\n",
      "Epoch 446/1000, Average Loss: 6.1392, Max Loss: 46.1676\n",
      "Epoch 448/1000, Average Loss: 8.3450, Max Loss: 101.7457\n",
      "Epoch 450/1000, Average Loss: 3.6955, Max Loss: 18.7149\n",
      "Epoch 452/1000, Average Loss: 9.2476, Max Loss: 72.0837\n",
      "Epoch 454/1000, Average Loss: 4.1930, Max Loss: 17.7798\n",
      "Epoch 456/1000, Average Loss: 3.8379, Max Loss: 11.9606\n",
      "Epoch 458/1000, Average Loss: 5.9849, Max Loss: 64.3186\n",
      "Epoch 460/1000, Average Loss: 11.0823, Max Loss: 68.3527\n",
      "Epoch 462/1000, Average Loss: 6.8233, Max Loss: 46.1045\n",
      "Epoch 464/1000, Average Loss: 7.3592, Max Loss: 83.6343\n",
      "Epoch 466/1000, Average Loss: 8.8794, Max Loss: 40.9877\n",
      "Epoch 468/1000, Average Loss: 5.0787, Max Loss: 15.6047\n",
      "Epoch 470/1000, Average Loss: 8.6010, Max Loss: 66.8089\n",
      "Epoch 472/1000, Average Loss: 11.0752, Max Loss: 70.9468\n",
      "Epoch 474/1000, Average Loss: 17.8394, Max Loss: 66.4129\n",
      "Epoch 476/1000, Average Loss: 3.9415, Max Loss: 12.4725\n",
      "Epoch 478/1000, Average Loss: 19.3626, Max Loss: 143.3087\n",
      "Epoch 480/1000, Average Loss: 20.9172, Max Loss: 109.3718\n",
      "Epoch 482/1000, Average Loss: 6.2998, Max Loss: 53.6378\n",
      "Epoch 484/1000, Average Loss: 18.7128, Max Loss: 90.1187\n",
      "Epoch 486/1000, Average Loss: 16.7090, Max Loss: 105.6150\n",
      "Epoch 488/1000, Average Loss: 6.9336, Max Loss: 32.9763\n",
      "Epoch 490/1000, Average Loss: 26.3535, Max Loss: 119.0116\n",
      "Epoch 492/1000, Average Loss: 18.0139, Max Loss: 84.9007\n",
      "Epoch 494/1000, Average Loss: 18.0246, Max Loss: 96.8935\n",
      "Epoch 496/1000, Average Loss: 14.1508, Max Loss: 63.0365\n",
      "Epoch 498/1000, Average Loss: 8.4401, Max Loss: 36.6778\n",
      "Epoch 500/1000, Average Loss: 8.5310, Max Loss: 76.2674\n",
      "Epoch 502/1000, Average Loss: 9.0385, Max Loss: 52.2533\n",
      "Epoch 504/1000, Average Loss: 18.1897, Max Loss: 120.9117\n",
      "Epoch 506/1000, Average Loss: 4.1048, Max Loss: 18.8922\n",
      "Epoch 508/1000, Average Loss: 9.2555, Max Loss: 97.9681\n",
      "Epoch 510/1000, Average Loss: 6.8412, Max Loss: 53.3537\n",
      "Epoch 512/1000, Average Loss: 8.8613, Max Loss: 52.9048\n",
      "Epoch 514/1000, Average Loss: 6.6456, Max Loss: 38.2067\n",
      "Epoch 516/1000, Average Loss: 4.6637, Max Loss: 20.3763\n",
      "Epoch 518/1000, Average Loss: 8.7430, Max Loss: 100.3972\n",
      "Epoch 520/1000, Average Loss: 7.0242, Max Loss: 49.0900\n",
      "Epoch 522/1000, Average Loss: 5.8620, Max Loss: 51.5781\n",
      "Epoch 524/1000, Average Loss: 5.7910, Max Loss: 32.2475\n",
      "Epoch 526/1000, Average Loss: 14.6324, Max Loss: 61.6494\n",
      "Epoch 528/1000, Average Loss: 13.5019, Max Loss: 58.7605\n",
      "Epoch 530/1000, Average Loss: 11.3602, Max Loss: 67.2481\n",
      "Epoch 532/1000, Average Loss: 9.9540, Max Loss: 81.7719\n",
      "Epoch 534/1000, Average Loss: 39.2819, Max Loss: 366.4226\n",
      "Epoch 536/1000, Average Loss: 21.3115, Max Loss: 150.0040\n",
      "Epoch 538/1000, Average Loss: 22.5845, Max Loss: 117.3094\n",
      "Epoch 540/1000, Average Loss: 11.5897, Max Loss: 49.9409\n",
      "Epoch 542/1000, Average Loss: 4.9920, Max Loss: 13.0906\n",
      "Epoch 544/1000, Average Loss: 13.6968, Max Loss: 108.9256\n",
      "Epoch 546/1000, Average Loss: 6.8997, Max Loss: 56.2814\n",
      "Epoch 548/1000, Average Loss: 3.1753, Max Loss: 6.1979\n",
      "Epoch 550/1000, Average Loss: 11.7287, Max Loss: 65.7018\n",
      "Epoch 552/1000, Average Loss: 11.0314, Max Loss: 106.0688\n",
      "Epoch 554/1000, Average Loss: 20.5631, Max Loss: 82.8068\n",
      "Epoch 556/1000, Average Loss: 9.5586, Max Loss: 53.3392\n",
      "Epoch 558/1000, Average Loss: 16.7188, Max Loss: 70.1042\n",
      "Epoch 560/1000, Average Loss: 6.1678, Max Loss: 63.0979\n",
      "Epoch 562/1000, Average Loss: 8.1723, Max Loss: 50.0871\n",
      "Epoch 564/1000, Average Loss: 12.6264, Max Loss: 63.9763\n",
      "Epoch 566/1000, Average Loss: 10.7762, Max Loss: 58.9995\n",
      "Epoch 568/1000, Average Loss: 2.8630, Max Loss: 7.8595\n",
      "Epoch 570/1000, Average Loss: 14.6575, Max Loss: 86.8749\n",
      "Epoch 572/1000, Average Loss: 10.7953, Max Loss: 120.0418\n",
      "Epoch 574/1000, Average Loss: 9.3904, Max Loss: 99.0537\n",
      "Epoch 576/1000, Average Loss: 24.4690, Max Loss: 151.6281\n",
      "Epoch 578/1000, Average Loss: 9.6118, Max Loss: 83.9104\n",
      "Epoch 580/1000, Average Loss: 4.1375, Max Loss: 11.1352\n",
      "Epoch 582/1000, Average Loss: 6.5055, Max Loss: 47.3795\n",
      "Epoch 584/1000, Average Loss: 5.3984, Max Loss: 49.5550\n",
      "Epoch 586/1000, Average Loss: 16.8504, Max Loss: 143.1574\n",
      "Epoch 588/1000, Average Loss: 12.8326, Max Loss: 59.3068\n",
      "Epoch 590/1000, Average Loss: 12.0988, Max Loss: 49.4136\n",
      "Epoch 592/1000, Average Loss: 13.6216, Max Loss: 75.9808\n",
      "Epoch 594/1000, Average Loss: 6.5477, Max Loss: 83.7235\n",
      "Epoch 596/1000, Average Loss: 3.4477, Max Loss: 19.7913\n",
      "Epoch 598/1000, Average Loss: 13.5845, Max Loss: 51.9806\n",
      "Epoch 600/1000, Average Loss: 13.1860, Max Loss: 74.2011\n",
      "Epoch 602/1000, Average Loss: 10.7594, Max Loss: 47.1608\n",
      "Epoch 604/1000, Average Loss: 4.1309, Max Loss: 6.6298\n",
      "Epoch 606/1000, Average Loss: 3.4242, Max Loss: 10.5128\n",
      "Epoch 608/1000, Average Loss: 8.6933, Max Loss: 55.1909\n",
      "Epoch 610/1000, Average Loss: 12.8035, Max Loss: 76.9292\n",
      "Epoch 612/1000, Average Loss: 9.4145, Max Loss: 76.4399\n",
      "Epoch 614/1000, Average Loss: 5.2826, Max Loss: 28.1987\n",
      "Epoch 616/1000, Average Loss: 17.4023, Max Loss: 118.8077\n",
      "Epoch 618/1000, Average Loss: 8.5008, Max Loss: 118.7011\n",
      "Epoch 620/1000, Average Loss: 15.6860, Max Loss: 101.2253\n",
      "Epoch 622/1000, Average Loss: 9.7146, Max Loss: 50.1189\n",
      "Epoch 624/1000, Average Loss: 3.2932, Max Loss: 5.6674\n",
      "Epoch 626/1000, Average Loss: 4.2064, Max Loss: 12.1768\n",
      "Epoch 628/1000, Average Loss: 4.3088, Max Loss: 30.5845\n",
      "Epoch 630/1000, Average Loss: 25.2296, Max Loss: 113.2638\n",
      "Epoch 632/1000, Average Loss: 9.3743, Max Loss: 52.9745\n",
      "Epoch 634/1000, Average Loss: 14.3067, Max Loss: 98.6377\n",
      "Epoch 636/1000, Average Loss: 4.8929, Max Loss: 16.2414\n",
      "Epoch 638/1000, Average Loss: 7.9110, Max Loss: 91.9168\n",
      "Epoch 640/1000, Average Loss: 2.6757, Max Loss: 4.4297\n",
      "Epoch 642/1000, Average Loss: 17.5223, Max Loss: 96.2195\n",
      "Epoch 644/1000, Average Loss: 22.6088, Max Loss: 74.9393\n",
      "Epoch 646/1000, Average Loss: 8.9190, Max Loss: 53.4835\n",
      "Epoch 648/1000, Average Loss: 27.6337, Max Loss: 113.1161\n",
      "Epoch 650/1000, Average Loss: 6.4510, Max Loss: 45.4803\n",
      "Epoch 652/1000, Average Loss: 11.9823, Max Loss: 109.4115\n",
      "Epoch 654/1000, Average Loss: 10.6443, Max Loss: 115.5503\n",
      "Epoch 656/1000, Average Loss: 12.8102, Max Loss: 66.2664\n",
      "Epoch 658/1000, Average Loss: 33.2769, Max Loss: 364.2254\n",
      "Epoch 660/1000, Average Loss: 7.9458, Max Loss: 91.7837\n",
      "Epoch 662/1000, Average Loss: 5.8877, Max Loss: 65.1140\n",
      "Epoch 664/1000, Average Loss: 15.3258, Max Loss: 119.6087\n",
      "Epoch 666/1000, Average Loss: 7.0814, Max Loss: 44.4262\n",
      "Epoch 668/1000, Average Loss: 3.7975, Max Loss: 15.3425\n",
      "Epoch 670/1000, Average Loss: 13.7683, Max Loss: 109.2983\n",
      "Epoch 672/1000, Average Loss: 3.3440, Max Loss: 9.4953\n",
      "Epoch 674/1000, Average Loss: 10.3570, Max Loss: 78.4481\n",
      "Epoch 676/1000, Average Loss: 7.4829, Max Loss: 50.0588\n",
      "Epoch 678/1000, Average Loss: 12.9706, Max Loss: 112.3892\n",
      "Epoch 680/1000, Average Loss: 4.9851, Max Loss: 16.5242\n",
      "Epoch 682/1000, Average Loss: 19.3886, Max Loss: 160.7296\n",
      "Epoch 684/1000, Average Loss: 18.2333, Max Loss: 83.2585\n",
      "Epoch 686/1000, Average Loss: 8.9824, Max Loss: 80.9239\n",
      "Epoch 688/1000, Average Loss: 11.1565, Max Loss: 120.3783\n",
      "Epoch 690/1000, Average Loss: 22.2505, Max Loss: 170.6637\n",
      "Epoch 692/1000, Average Loss: 5.4678, Max Loss: 21.2186\n",
      "Epoch 694/1000, Average Loss: 6.8699, Max Loss: 85.4496\n",
      "Epoch 696/1000, Average Loss: 9.5948, Max Loss: 62.6076\n",
      "Epoch 698/1000, Average Loss: 7.5180, Max Loss: 69.3344\n",
      "Epoch 700/1000, Average Loss: 6.3229, Max Loss: 27.7428\n",
      "Epoch 702/1000, Average Loss: 12.5037, Max Loss: 105.3284\n",
      "Epoch 704/1000, Average Loss: 14.4616, Max Loss: 82.0784\n",
      "Epoch 706/1000, Average Loss: 14.5632, Max Loss: 149.3197\n",
      "Epoch 708/1000, Average Loss: 10.7733, Max Loss: 86.2440\n",
      "Epoch 710/1000, Average Loss: 4.9481, Max Loss: 14.2892\n",
      "Epoch 712/1000, Average Loss: 10.3005, Max Loss: 106.6056\n",
      "Epoch 714/1000, Average Loss: 4.7510, Max Loss: 42.2433\n",
      "Epoch 716/1000, Average Loss: 13.9790, Max Loss: 108.8137\n",
      "Epoch 718/1000, Average Loss: 6.5094, Max Loss: 23.5865\n",
      "Epoch 720/1000, Average Loss: 12.9564, Max Loss: 90.9574\n",
      "Epoch 722/1000, Average Loss: 9.4389, Max Loss: 129.4345\n",
      "Epoch 724/1000, Average Loss: 9.8733, Max Loss: 95.8785\n",
      "Epoch 726/1000, Average Loss: 21.8163, Max Loss: 155.4340\n",
      "Epoch 728/1000, Average Loss: 9.7376, Max Loss: 64.7422\n",
      "Epoch 730/1000, Average Loss: 18.7221, Max Loss: 85.2843\n",
      "Epoch 732/1000, Average Loss: 13.6967, Max Loss: 50.7094\n",
      "Epoch 734/1000, Average Loss: 7.5317, Max Loss: 63.8745\n",
      "Epoch 736/1000, Average Loss: 5.8549, Max Loss: 39.7526\n",
      "Epoch 738/1000, Average Loss: 8.8705, Max Loss: 86.1618\n",
      "Epoch 740/1000, Average Loss: 10.7208, Max Loss: 97.5560\n",
      "Epoch 742/1000, Average Loss: 16.5319, Max Loss: 76.9656\n",
      "Epoch 744/1000, Average Loss: 6.8408, Max Loss: 63.3708\n",
      "Epoch 746/1000, Average Loss: 8.7745, Max Loss: 60.2514\n",
      "Epoch 748/1000, Average Loss: 4.1670, Max Loss: 10.5951\n",
      "Epoch 750/1000, Average Loss: 20.4011, Max Loss: 106.3168\n",
      "Epoch 752/1000, Average Loss: 21.3742, Max Loss: 140.8099\n",
      "Epoch 754/1000, Average Loss: 17.0635, Max Loss: 57.4489\n",
      "Epoch 756/1000, Average Loss: 15.9046, Max Loss: 142.1827\n",
      "Epoch 758/1000, Average Loss: 11.5082, Max Loss: 102.9811\n",
      "Epoch 760/1000, Average Loss: 8.3787, Max Loss: 55.0965\n",
      "Epoch 762/1000, Average Loss: 3.2224, Max Loss: 3.9848\n",
      "Epoch 764/1000, Average Loss: 3.5950, Max Loss: 8.2832\n",
      "Epoch 766/1000, Average Loss: 3.5495, Max Loss: 10.8063\n",
      "Epoch 768/1000, Average Loss: 6.0824, Max Loss: 42.1853\n",
      "Epoch 770/1000, Average Loss: 5.5747, Max Loss: 36.3792\n",
      "Epoch 772/1000, Average Loss: 16.3595, Max Loss: 51.5475\n",
      "Epoch 774/1000, Average Loss: 13.0298, Max Loss: 94.0030\n",
      "Epoch 776/1000, Average Loss: 8.6783, Max Loss: 79.6227\n",
      "Epoch 778/1000, Average Loss: 5.4845, Max Loss: 29.4963\n",
      "Epoch 780/1000, Average Loss: 3.7971, Max Loss: 19.0269\n",
      "Epoch 782/1000, Average Loss: 10.3864, Max Loss: 105.8395\n",
      "Epoch 784/1000, Average Loss: 2.9993, Max Loss: 7.4422\n",
      "Epoch 786/1000, Average Loss: 3.5419, Max Loss: 12.4282\n",
      "Epoch 788/1000, Average Loss: 8.6565, Max Loss: 50.2990\n",
      "Epoch 790/1000, Average Loss: 2.0620, Max Loss: 3.3258\n",
      "Epoch 792/1000, Average Loss: 9.1807, Max Loss: 59.3061\n",
      "Epoch 794/1000, Average Loss: 6.9490, Max Loss: 39.9218\n",
      "Epoch 796/1000, Average Loss: 5.5331, Max Loss: 20.3855\n",
      "Epoch 798/1000, Average Loss: 6.7532, Max Loss: 53.4927\n",
      "Epoch 800/1000, Average Loss: 2.4405, Max Loss: 7.7804\n",
      "Epoch 802/1000, Average Loss: 5.1704, Max Loss: 15.2911\n",
      "Epoch 804/1000, Average Loss: 18.2119, Max Loss: 120.8875\n",
      "Epoch 806/1000, Average Loss: 2.6872, Max Loss: 5.7038\n",
      "Epoch 808/1000, Average Loss: 2.6808, Max Loss: 3.1584\n",
      "Epoch 810/1000, Average Loss: 2.5001, Max Loss: 4.0248\n",
      "Epoch 812/1000, Average Loss: 4.5684, Max Loss: 22.4983\n",
      "Epoch 814/1000, Average Loss: 9.0545, Max Loss: 131.2072\n",
      "Epoch 816/1000, Average Loss: 3.3422, Max Loss: 6.9159\n",
      "Epoch 818/1000, Average Loss: 11.0721, Max Loss: 73.0975\n",
      "Epoch 820/1000, Average Loss: 9.7883, Max Loss: 117.7493\n",
      "Epoch 822/1000, Average Loss: 12.7526, Max Loss: 106.2747\n",
      "Epoch 824/1000, Average Loss: 8.1414, Max Loss: 80.4622\n",
      "Epoch 826/1000, Average Loss: 8.2380, Max Loss: 54.1325\n",
      "Epoch 828/1000, Average Loss: 7.4232, Max Loss: 41.5242\n",
      "Epoch 830/1000, Average Loss: 9.7554, Max Loss: 46.4691\n",
      "Epoch 832/1000, Average Loss: 11.3321, Max Loss: 127.3818\n",
      "Epoch 834/1000, Average Loss: 12.2170, Max Loss: 106.7026\n",
      "Epoch 836/1000, Average Loss: 18.3536, Max Loss: 59.4684\n",
      "Epoch 838/1000, Average Loss: 5.3360, Max Loss: 39.2688\n",
      "Epoch 840/1000, Average Loss: 6.6122, Max Loss: 51.4219\n",
      "Epoch 842/1000, Average Loss: 6.3246, Max Loss: 56.7625\n",
      "Epoch 844/1000, Average Loss: 6.8471, Max Loss: 38.7193\n",
      "Epoch 846/1000, Average Loss: 9.9020, Max Loss: 91.4560\n",
      "Epoch 848/1000, Average Loss: 14.2659, Max Loss: 60.0918\n",
      "Epoch 850/1000, Average Loss: 12.5130, Max Loss: 105.2910\n",
      "Epoch 852/1000, Average Loss: 11.8789, Max Loss: 57.5289\n",
      "Epoch 854/1000, Average Loss: 13.9669, Max Loss: 98.9051\n",
      "Epoch 856/1000, Average Loss: 12.3027, Max Loss: 153.7397\n",
      "Epoch 858/1000, Average Loss: 14.7762, Max Loss: 88.3596\n",
      "Epoch 860/1000, Average Loss: 3.8057, Max Loss: 15.0821\n",
      "Epoch 862/1000, Average Loss: 14.6652, Max Loss: 133.3866\n",
      "Epoch 864/1000, Average Loss: 14.5425, Max Loss: 117.3322\n",
      "Epoch 866/1000, Average Loss: 10.0125, Max Loss: 93.6184\n",
      "Epoch 868/1000, Average Loss: 10.9742, Max Loss: 74.7430\n",
      "Epoch 870/1000, Average Loss: 5.8405, Max Loss: 35.4984\n",
      "Epoch 872/1000, Average Loss: 16.8229, Max Loss: 113.0468\n",
      "Epoch 874/1000, Average Loss: 12.4794, Max Loss: 102.7985\n",
      "Epoch 876/1000, Average Loss: 2.2832, Max Loss: 2.9588\n",
      "Epoch 878/1000, Average Loss: 19.7117, Max Loss: 145.9969\n",
      "Epoch 880/1000, Average Loss: 14.4976, Max Loss: 69.1143\n",
      "Epoch 882/1000, Average Loss: 11.0790, Max Loss: 80.7467\n",
      "Epoch 884/1000, Average Loss: 14.4496, Max Loss: 49.4678\n",
      "Epoch 886/1000, Average Loss: 12.8111, Max Loss: 65.6596\n",
      "Epoch 888/1000, Average Loss: 7.7083, Max Loss: 58.9467\n",
      "Epoch 890/1000, Average Loss: 16.0636, Max Loss: 45.2344\n",
      "Epoch 892/1000, Average Loss: 9.5067, Max Loss: 71.4879\n",
      "Epoch 894/1000, Average Loss: 15.2241, Max Loss: 116.6650\n",
      "Epoch 896/1000, Average Loss: 9.8320, Max Loss: 118.7253\n",
      "Epoch 898/1000, Average Loss: 4.5771, Max Loss: 9.2915\n",
      "Epoch 900/1000, Average Loss: 15.1527, Max Loss: 64.1732\n",
      "Epoch 902/1000, Average Loss: 23.0132, Max Loss: 153.4138\n",
      "Epoch 904/1000, Average Loss: 6.5540, Max Loss: 46.1436\n",
      "Epoch 906/1000, Average Loss: 13.1534, Max Loss: 76.6753\n",
      "Epoch 908/1000, Average Loss: 11.3708, Max Loss: 95.2031\n",
      "Epoch 910/1000, Average Loss: 7.6671, Max Loss: 72.1844\n",
      "Epoch 912/1000, Average Loss: 4.5417, Max Loss: 31.9253\n",
      "Epoch 914/1000, Average Loss: 2.4434, Max Loss: 6.1077\n",
      "Epoch 916/1000, Average Loss: 8.4831, Max Loss: 83.8833\n",
      "Epoch 918/1000, Average Loss: 4.3951, Max Loss: 23.3830\n",
      "Epoch 920/1000, Average Loss: 16.2359, Max Loss: 58.9336\n",
      "Epoch 922/1000, Average Loss: 10.9776, Max Loss: 66.5933\n",
      "Epoch 924/1000, Average Loss: 4.3387, Max Loss: 37.0547\n",
      "Epoch 926/1000, Average Loss: 6.5887, Max Loss: 45.7278\n",
      "Epoch 928/1000, Average Loss: 3.3819, Max Loss: 17.2460\n",
      "Epoch 930/1000, Average Loss: 14.4157, Max Loss: 58.4734\n",
      "Epoch 932/1000, Average Loss: 15.1485, Max Loss: 66.9826\n",
      "Epoch 934/1000, Average Loss: 9.6795, Max Loss: 49.1330\n",
      "Epoch 936/1000, Average Loss: 14.8729, Max Loss: 80.2288\n",
      "Epoch 938/1000, Average Loss: 5.2910, Max Loss: 49.5775\n",
      "Epoch 940/1000, Average Loss: 37.9977, Max Loss: 364.0967\n",
      "Epoch 942/1000, Average Loss: 7.0436, Max Loss: 50.5670\n",
      "Epoch 944/1000, Average Loss: 6.4547, Max Loss: 44.8654\n",
      "Epoch 946/1000, Average Loss: 6.7108, Max Loss: 54.4860\n",
      "Epoch 948/1000, Average Loss: 12.0131, Max Loss: 126.7127\n",
      "Epoch 950/1000, Average Loss: 40.3669, Max Loss: 365.2862\n",
      "Epoch 952/1000, Average Loss: 10.4259, Max Loss: 58.2160\n",
      "Epoch 954/1000, Average Loss: 8.9414, Max Loss: 51.7294\n",
      "Epoch 956/1000, Average Loss: 9.9290, Max Loss: 115.6749\n",
      "Epoch 958/1000, Average Loss: 15.5205, Max Loss: 117.1429\n",
      "Epoch 960/1000, Average Loss: 10.9396, Max Loss: 51.3390\n",
      "Epoch 962/1000, Average Loss: 16.1365, Max Loss: 93.7621\n",
      "Epoch 964/1000, Average Loss: 12.9569, Max Loss: 61.8836\n",
      "Epoch 966/1000, Average Loss: 5.3059, Max Loss: 53.8955\n",
      "Epoch 968/1000, Average Loss: 9.7843, Max Loss: 52.2377\n",
      "Epoch 970/1000, Average Loss: 9.9847, Max Loss: 72.8102\n",
      "Epoch 972/1000, Average Loss: 7.5217, Max Loss: 38.8587\n",
      "Epoch 974/1000, Average Loss: 8.6362, Max Loss: 52.8532\n",
      "Epoch 976/1000, Average Loss: 6.0682, Max Loss: 38.6968\n",
      "Epoch 978/1000, Average Loss: 7.4393, Max Loss: 45.9973\n",
      "Epoch 980/1000, Average Loss: 13.9998, Max Loss: 75.2430\n",
      "Epoch 982/1000, Average Loss: 13.7404, Max Loss: 115.4664\n",
      "Epoch 984/1000, Average Loss: 3.3986, Max Loss: 16.8292\n",
      "Epoch 986/1000, Average Loss: 10.0652, Max Loss: 56.7609\n",
      "Epoch 988/1000, Average Loss: 9.2505, Max Loss: 86.7188\n",
      "Epoch 990/1000, Average Loss: 8.8929, Max Loss: 111.8267\n",
      "Epoch 992/1000, Average Loss: 17.4506, Max Loss: 134.3388\n",
      "Epoch 994/1000, Average Loss: 19.5401, Max Loss: 110.5786\n",
      "Epoch 996/1000, Average Loss: 12.8185, Max Loss: 95.4595\n",
      "Epoch 998/1000, Average Loss: 45.1883, Max Loss: 365.2802\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "sequence_length = 5\n",
    "temporal_model = GraphStatePredictor(hidden_channels=64, out_channels=4, num_heads=2, num_layers=3, context_window=5, data=all_data_sets[0]['data']['graph'][0])\n",
    "# Initialize optimizer and loss function\n",
    "predictor_optimizer = torch.optim.Adam(temporal_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "temporal_model = temporal_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "temporal_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    max_loss = 0.0\n",
    "    # Create training sequences from all_data_sets\n",
    "    for i in range(len(dataset['graph']) - sequence_length):\n",
    "        # Sample from random index in the current dataset\n",
    "        if len(dataset['graph']) > sequence_length:\n",
    "            random_index = np.random.randint(0, len(dataset['graph']) - sequence_length)\n",
    "            # print(random_index)\n",
    "        else:\n",
    "            random_index = 0\n",
    "        \n",
    "        # Use the random index for both input sequence and target\n",
    "        input_sequence = dataset['graph'][random_index:random_index+sequence_length]\n",
    "        target_graph = dataset['graph'][random_index+sequence_length]\n",
    "        # # Get input sequence and target\n",
    "        # input_sequence = dataset['graph'][i:i+sequence_length]\n",
    "        # target_graph = dataset['graph'][i+sequence_length]\n",
    "        # Move data to device\n",
    "        input_sequence = [graph.to(device) for graph in input_sequence]\n",
    "        target_dict = {node_type: features.to(device) for node_type, features in target_graph.x_dict.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        predictor_optimizer.zero_grad()\n",
    "        predictions = temporal_model(input_sequence)\n",
    "        \n",
    "        # Calculate loss for each node type\n",
    "        total_loss = 0.0\n",
    "        for node_type in predictions.keys():\n",
    "            loss = criterion(predictions[node_type], target_dict[node_type])\n",
    "            total_loss += loss\n",
    "            max_loss = max(max_loss, loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        predictor_optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Break if we have enough training samples\n",
    "        if num_batches >= 20:  # Limit training samples per epoch\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Average Loss: {avg_loss:.4f}, Max Loss: {max_loss:.4f}\")\n",
    "        # print(f\"index used: {index}, results of this index: {dataset['Reward']}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "60469a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: tensor([[nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[ 1.0000, 13.0000,  0.2500,  7.0000,  0.0000]], device='cuda:0')\n",
      "prediction: tensor([[nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[15.0000,  5.0000,  2.0000,  0.0211]], device='cuda:0')\n",
      "prediction: tensor([[nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[1., 0., 0., 0.]], device='cuda:0')\n",
      "prediction: tensor([[nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[1.0000, 0.0000, 0.0000, 0.1728, 5.0000, 0.0000]], device='cuda:0')\n",
      "prediction: tensor([[nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[1.0000, 0.0000, 1.0000, 0.1707, 4.0000, 0.0000]], device='cuda:0')\n",
      "prediction: tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "target: tensor([[ 0.0000,  0.0000,  4.0000,  8.9316,  0.1053,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  4.0000, 20.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  4.0000, 24.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  4.0000, 28.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  4.0000, 32.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "Test Loss: nan\n"
     ]
    }
   ],
   "source": [
    "temporal_model.eval()\n",
    "eval_dataset = torch.load('../data/complex_line_graph_n_assemblies5_waiting_time5.pt', weights_only=False)\n",
    "with torch.no_grad():\n",
    "    test_input_sequence = [graph.to(device) for graph in eval_dataset['graph'][7:7+sequence_length]]\n",
    "    test_target_graph = eval_dataset['graph'][sequence_length]\n",
    "    test_target_dict = {node_type: features.to(device) for node_type, features in test_target_graph.x_dict.items()}\n",
    "    \n",
    "    test_predictions = temporal_model(test_input_sequence)\n",
    "    \n",
    "    # Calculate test loss\n",
    "    test_loss = 0.0\n",
    "    for node_type in test_predictions.keys():\n",
    "        loss = criterion(test_predictions[node_type], test_target_dict[node_type])\n",
    "        print(\"prediction:\", test_predictions[node_type])\n",
    "        print(\"target:\", test_target_dict[node_type])\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b3ca533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Average Loss: 781.4375\n",
      "index used: 16, results of this index: 111.75\n",
      "Epoch 10/1000, Average Loss: 21.5454\n",
      "index used: 2, results of this index: -1.3333333333333333\n",
      "Epoch 20/1000, Average Loss: 12.9138\n",
      "index used: 23, results of this index: -1.0\n",
      "Epoch 30/1000, Average Loss: 10.3822\n",
      "index used: 15, results of this index: -1.3333333333333333\n",
      "Epoch 40/1000, Average Loss: 562.6829\n",
      "index used: 21, results of this index: 53.5\n",
      "Epoch 50/1000, Average Loss: 2.3937\n",
      "index used: 12, results of this index: -1.0\n",
      "Epoch 60/1000, Average Loss: 7.1902\n",
      "index used: 4, results of this index: -1.3333333333333333\n",
      "Epoch 70/1000, Average Loss: 2.4336\n",
      "index used: 19, results of this index: -1.3333333333333333\n",
      "Epoch 80/1000, Average Loss: 7.0778\n",
      "index used: 3, results of this index: -1.0\n",
      "Epoch 90/1000, Average Loss: 80.8783\n",
      "index used: 0, results of this index: -7.5\n",
      "Epoch 100/1000, Average Loss: 220.5285\n",
      "index used: 5, results of this index: 161.5\n",
      "Epoch 110/1000, Average Loss: 102.3174\n",
      "index used: 22, results of this index: -7.5\n",
      "Epoch 120/1000, Average Loss: 64.6348\n",
      "index used: 8, results of this index: -7.5\n",
      "Epoch 130/1000, Average Loss: 82.3528\n",
      "index used: 0, results of this index: -7.5\n",
      "Epoch 140/1000, Average Loss: 1935.6174\n",
      "index used: 10, results of this index: -38.0\n",
      "Epoch 150/1000, Average Loss: 82.5987\n",
      "index used: 9, results of this index: -1.0\n",
      "Epoch 160/1000, Average Loss: 78.2057\n",
      "index used: 14, results of this index: -1.0\n",
      "Epoch 170/1000, Average Loss: 229.2530\n",
      "index used: 5, results of this index: 161.5\n",
      "Epoch 180/1000, Average Loss: 29.4236\n",
      "index used: 18, results of this index: -1.3333333333333333\n",
      "Epoch 190/1000, Average Loss: 593.5587\n",
      "index used: 16, results of this index: 111.75\n",
      "Epoch 200/1000, Average Loss: 2094.7853\n",
      "index used: 10, results of this index: -38.0\n",
      "Epoch 210/1000, Average Loss: 112.5324\n",
      "index used: 20, results of this index: -7.5\n",
      "Epoch 220/1000, Average Loss: 70.3391\n",
      "index used: 9, results of this index: -1.0\n",
      "Epoch 230/1000, Average Loss: 32.4124\n",
      "index used: 19, results of this index: -1.3333333333333333\n",
      "Epoch 240/1000, Average Loss: 69.0465\n",
      "index used: 8, results of this index: -7.5\n",
      "Epoch 250/1000, Average Loss: 64.0759\n",
      "index used: 22, results of this index: -7.5\n",
      "Epoch 260/1000, Average Loss: 163.7278\n",
      "index used: 8, results of this index: -7.5\n",
      "Epoch 270/1000, Average Loss: 102.4095\n",
      "index used: 13, results of this index: -7.5\n",
      "Epoch 280/1000, Average Loss: 627.5245\n",
      "index used: 21, results of this index: 53.5\n",
      "Epoch 290/1000, Average Loss: 105.5312\n",
      "index used: 22, results of this index: -7.5\n",
      "Epoch 300/1000, Average Loss: 56.7134\n",
      "index used: 14, results of this index: -1.0\n",
      "Epoch 310/1000, Average Loss: 45.2994\n",
      "index used: 19, results of this index: -1.3333333333333333\n",
      "Epoch 320/1000, Average Loss: 74.4734\n",
      "index used: 13, results of this index: -7.5\n",
      "Epoch 330/1000, Average Loss: 55.1720\n",
      "index used: 13, results of this index: -7.5\n",
      "Epoch 340/1000, Average Loss: 914.0450\n",
      "index used: 21, results of this index: 53.5\n",
      "Epoch 350/1000, Average Loss: 106.6429\n",
      "index used: 25, results of this index: 91.0\n",
      "Epoch 360/1000, Average Loss: 170.8536\n",
      "index used: 17, results of this index: 136.25\n",
      "Epoch 370/1000, Average Loss: 33.6918\n",
      "index used: 15, results of this index: -1.3333333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m predictor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Calculate loss for each node type\u001b[39;00m\n\u001b[1;32m     51\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[118], line 72\u001b[0m, in \u001b[0;36mGraphStatePredictor.forward\u001b[0;34m(self, graph_sequence)\u001b[0m\n\u001b[1;32m     69\u001b[0m     encoded_sequence[node_type] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m graph_sequence:\n\u001b[0;32m---> 72\u001b[0m     encoded_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_single_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_type \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mnode_types:\n\u001b[1;32m     74\u001b[0m         encoded_sequence[node_type]\u001b[38;5;241m.\u001b[39mappend(encoded_graph[node_type])\n",
      "Cell \u001b[0;32mIn[118], line 53\u001b[0m, in \u001b[0;36mGraphStatePredictor.encode_single_graph\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Apply HGT convolutions\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_convs:\n\u001b[0;32m---> 53\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch_geometric/nn/conv/hgt_conv.py:195\u001b[0m, in \u001b[0;36mHGTConv.forward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m    191\u001b[0m q, dst_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cat(q_dict)\n\u001b[1;32m    192\u001b[0m k, v, src_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_src_node_feat(\n\u001b[1;32m    193\u001b[0m     k_dict, v_dict, edge_index_dict)\n\u001b[0;32m--> 195\u001b[0m edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_bipartite_edge_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_rel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, k\u001b[38;5;241m=\u001b[39mk, q\u001b[38;5;241m=\u001b[39mq, v\u001b[38;5;241m=\u001b[39mv, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Reconstruct output node embeddings dict:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch_geometric/utils/hetero.py:134\u001b[0m, in \u001b[0;36mconstruct_bipartite_edge_index\u001b[0;34m(edge_index_dict, src_offset_dict, dst_offset_dict, edge_attr_dict, num_nodes)\u001b[0m\n\u001b[1;32m    132\u001b[0m             value \u001b[38;5;241m=\u001b[39m edge_attr_dict[edge_type]\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m             value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         edge_attrs\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    137\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(edge_indices, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Training setup for GraphStatePredictor\n",
    "# import torch.nn as nn\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# # Training parameters\n",
    "# num_epochs = 1000\n",
    "# learning_rate = 1e-3\n",
    "# batch_size = 16\n",
    "# sequence_length = 5\n",
    "# temporal_model = GraphStatePredictor(hidden_channels=64, out_channels=4, num_heads=2, num_layers=3, context_window=5, data=all_data_sets[0]['data']['graph'][0])\n",
    "# # Initialize optimizer and loss function\n",
    "# predictor_optimizer = torch.optim.Adam(temporal_model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Move model to device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# temporal_model = temporal_model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# temporal_model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     num_batches = 0\n",
    "#     index = np.random.randint(0, len(all_data_sets)-1)\n",
    "#     data_set = all_data_sets[index]['data']\n",
    "#     # Create training sequences from all_data_sets\n",
    "#     for i in range(len(data_set['graph']) - sequence_length):\n",
    "#         # Sample from random index in the current dataset\n",
    "#         if len(data_set['graph']) > sequence_length:\n",
    "#             random_index = np.random.randint(0, len(data_set['graph']) - sequence_length)\n",
    "#             # print(random_index)\n",
    "#         else:\n",
    "#             random_index = 0\n",
    "        \n",
    "#         # Use the random index for both input sequence and target\n",
    "#         input_sequence = data_set['graph'][random_index:random_index+sequence_length]\n",
    "#         target_graph = data_set['graph'][random_index+sequence_length]\n",
    "#         # # Get input sequence and target\n",
    "#         # input_sequence = data_set['graph'][i:i+sequence_length]\n",
    "#         # target_graph = data_set['graph'][i+sequence_length]\n",
    "#         # Move data to device\n",
    "#         input_sequence = [graph.to(device) for graph in input_sequence]\n",
    "#         target_dict = {node_type: features.to(device) for node_type, features in target_graph.x_dict.items()}\n",
    "        \n",
    "#         # Forward pass\n",
    "#         predictor_optimizer.zero_grad()\n",
    "#         predictions = temporal_model(input_sequence)\n",
    "        \n",
    "#         # Calculate loss for each node type\n",
    "#         total_loss = 0.0\n",
    "#         for node_type in predictions.keys():\n",
    "#             loss = criterion(predictions[node_type], target_dict[node_type])\n",
    "#             total_loss += loss\n",
    "        \n",
    "#         # Backward pass\n",
    "#         total_loss.backward()\n",
    "#         predictor_optimizer.step()\n",
    "        \n",
    "#         epoch_loss += total_loss.item()\n",
    "#         num_batches += 1\n",
    "        \n",
    "#         # Break if we have enough training samples\n",
    "#         if num_batches >= 20:  # Limit training samples per epoch\n",
    "#             break\n",
    "    \n",
    "#     avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "#         print(f\"index used: {index}, results of this index: {data_set['Reward']}\")\n",
    "\n",
    "# print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014d1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1f0ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = temporal_model(data_set['graph'][0:5])  # sequence of identical graphs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b409f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Magazine': tensor([[ 0.3500,  0.3693,  0.2771, -0.2887,  0.0171]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'WorkerPool': tensor([[-0.0949,  0.1112,  0.5149,  0.8446]], grad_fn=<AddmmBackward0>),\n",
       " 'Sink': tensor([[-0.1366,  0.8128,  0.9061, -0.7785]], grad_fn=<AddmmBackward0>),\n",
       " 'Source': tensor([[ 0.4085, -0.7399,  0.1885,  0.0829,  0.9640, -0.2505]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'Switch': tensor([[ 0.8235, -0.0293,  0.0483, -0.5626, -1.3464, -0.1941]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'Assembly': tensor([[-0.6272,  0.3712,  0.6551, -0.6825,  0.1140,  0.0876, -0.3545],\n",
       "         [-0.6307,  0.0448,  0.3735, -0.8329,  0.5517,  0.0739, -0.3345],\n",
       "         [-0.5975,  0.2197,  0.4470, -0.8527,  0.0972,  0.4500, -0.2877],\n",
       "         [-0.6554,  0.1610,  0.7087, -0.8310, -0.2515,  0.3587, -0.1982],\n",
       "         [-0.5200,  0.2731,  0.7815, -0.9731, -0.1957,  0.6623, -0.3262],\n",
       "         [-0.7906,  0.0555,  0.9174, -0.8720, -0.2623,  0.8126, -0.0245]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "592fdaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Magazine': tensor([[ 0.3323,  0.4991,  0.2458, -0.2821, -0.4450]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'WorkerPool': tensor([[-0.2104, -0.1976,  0.3826,  0.7240]], grad_fn=<AddmmBackward0>),\n",
       " 'Sink': tensor([[-0.5294,  0.5715,  0.7074, -0.7576]], grad_fn=<AddmmBackward0>),\n",
       " 'Source': tensor([[ 0.2296, -0.9456,  0.1153, -0.0331,  0.6303,  0.0750]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'Switch': tensor([[ 0.8308, -0.2942, -0.0924, -0.5821, -1.4638, -0.0118]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'Assembly': tensor([[-0.5084,  0.2951,  0.3163, -1.1244,  0.0074,  0.3913, -0.4088],\n",
       "         [-0.5381,  0.2817,  0.6267, -0.9835, -0.3033,  0.6450, -0.2259],\n",
       "         [-0.6545,  0.0997,  0.5190, -0.9499, -0.0375,  0.8731, -0.2883],\n",
       "         [-0.5927, -0.0076,  0.6942, -0.9549, -0.1295,  0.5250, -0.3203],\n",
       "         [-0.6437,  0.1522,  0.7409, -1.0244, -0.2535,  0.5927, -0.0028],\n",
       "         [-0.6020,  0.4245,  0.8681, -0.8637, -0.1611,  0.7473, -0.1220]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ab606a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Magazine', 'WorkerPool', 'Sink', 'Source', 'Switch', 'Assembly'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "871f047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['Source'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d9bf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_train.action_space.nvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f685945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ppo import Agent\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0931378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, input_feat_dim):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_feat_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_feat_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.action_space.nvec[0]), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceafef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define undefined variables (without using args)\n",
    "env_id = \"WaitingTime-v0\"\n",
    "exp_name = \"ppo_experiment\"\n",
    "seed = 42\n",
    "torch_deterministic = True\n",
    "cuda = True\n",
    "track = False\n",
    "wandb_project_name = \"ppo_project\"\n",
    "wandb_entity = None\n",
    "\n",
    "num_envs = 1\n",
    "num_steps = 128\n",
    "num_minibatches = 4\n",
    "total_timesteps = 100_000\n",
    "learning_rate = 2.5e-4\n",
    "anneal_lr = True\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "update_epochs = 4\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "clip_vloss = True\n",
    "norm_adv = True\n",
    "target_kl = None\n",
    "\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "num_iterations = total_timesteps // batch_size\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "if track:\n",
    "    import wandb\n",
    "    import types\n",
    "\n",
    "    wandb.init(\n",
    "            project=wandb_project_name,\n",
    "            entity=wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [make_env(env_id, i, capture_video, run_name) for i in range(num_envs)],\n",
    "# )\n",
    "line = WaitingTime(use_graph_as_states=True)\n",
    "envs = make_stacked_vec_env(\n",
    "    line=line,\n",
    "    simulation_end=100+1,\n",
    "    reward=\"parts\",\n",
    "    n_envs=1,\n",
    "    n_stack=1,\n",
    ")\n",
    "# assert isinstance(envs.action_space, gym.spaces.MultiDiscrete), \"only discrete action space is supported\"\n",
    "graph_encoder_feature_dim = 64\n",
    "graph_encoder = HGT(hidden_channels=graph_encoder_feature_dim, out_channels=4, num_heads=2, num_layers=1)\n",
    "agent = Agent(envs, graph_encoder_feature_dim).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "# Calculate the total encoded feature dimension\n",
    "# This should match the concatenated output from your graph encoder\n",
    "total_encoded_dim = 64 #* len(state.node_types)  # assuming each node type gets encoded to 64 dims\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs, total_encoded_dim)).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs), dtype=torch.long).to(device)  # Single action, not vector\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "\n",
    "# Move graph_encoder to device\n",
    "graph_encoder = graph_encoder.to(device)\n",
    "\n",
    "# Initial encoding of the hetero graph observation\n",
    "# Move HeteroData to device\n",
    "next_obs = next_obs.to(device)\n",
    "next_obs_graph = graph_encoder(next_obs.x_dict, next_obs.edge_index_dict)\n",
    "# Concatenate all node type embeddings into a single feature vector\n",
    "next_obs_encoded = next_obs_graph['Source'][1].unsqueeze(0)\n",
    "\n",
    "next_obs = next_obs_encoded  # This should be the encoded features, not raw HeteroData\n",
    "next_done = torch.zeros(num_envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65004025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73481ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HeteroData' has no attribute 'get_observations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m next_obs_raw, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m next_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n\u001b[1;32m     23\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/agile_stem/lineflow/lineflow/simulation/environment.py:194\u001b[0m, in \u001b[0;36mLineSimulation.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    191\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observations_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    197\u001b[0m     reward \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mget_n_parts_produced() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_parts) \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mscrap_factor\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mget_n_scrap_parts() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_scrap_parts)\n",
      "File \u001b[0;32m~/agile_stem/lineflow/lineflow/simulation/environment.py:256\u001b[0m, in \u001b[0;36mLineSimulation._get_observations_as_tensor\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_observations_as_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m--> 256\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observations\u001b[49m(lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, include_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch_geometric/data/hetero_data.py:162\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dict$\u001b[39m\u001b[38;5;124m'\u001b[39m, key)):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'get_observations'"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, num_iterations + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if anneal_lr:\n",
    "        frac = 1.0 - (iteration - 1.0) / num_iterations\n",
    "        lrnow = frac * learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs_raw, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        \n",
    "        # Process the new HeteroData observation through graph encoder\n",
    "        # Move HeteroData to device\n",
    "        next_obs_raw = next_obs_raw.to(device)\n",
    "        next_obs_graph = graph_encoder(next_obs_raw.x_dict, next_obs_raw.edge_index_dict)\n",
    "        # Concatenate all node type embeddings into a single feature vector\n",
    "        next_obs = next_obs_graph['Source'][1].unsqueeze(0)\n",
    "    \n",
    "        next_done = torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1, total_encoded_dim))  # Use the correct encoded dimension\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)  # Single action dimension\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if target_kl is not None and approx_kl > target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382d4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agilestem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
