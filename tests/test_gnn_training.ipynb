{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f210e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.21)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import ast\n",
    "import argparse\n",
    "import os\n",
    "from lineflow.helpers import get_device\n",
    "from lineflow.learning.helpers import (\n",
    "    make_stacked_vec_env,\n",
    ")\n",
    "from lineflow.learning.curriculum import CurriculumLearningCallback\n",
    "from lineflow.examples import (\n",
    "    WaitingTime,\n",
    ")\n",
    "\n",
    "\n",
    "from wandb.integration.sb3 import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bac812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = WaitingTime(use_graph_as_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ce38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = make_stacked_vec_env(\n",
    "        line=line,\n",
    "        simulation_end=100+1,\n",
    "        reward=\"parts\",\n",
    "        n_envs=1,\n",
    "        n_stack=1,\n",
    "        use_graph = True,\n",
    "        track_states=['S_component']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce124827",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env_train.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b323e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  Source={ x=[2, 6] },\n",
       "  Assembly={ x=[1, 7] },\n",
       "  Sink={ x=[1, 4] },\n",
       "  (Source, connects_to, Assembly)={\n",
       "    edge_index=[2, 2],\n",
       "    edge_attr=[2, 2],\n",
       "  },\n",
       "  (Assembly, connects_to, Sink)={\n",
       "    edge_index=[2, 1],\n",
       "    edge_attr=[1, 2],\n",
       "  },\n",
       "  (Source, self_loop, Source)={\n",
       "    edge_index=[2, 2],\n",
       "    edge_attr=[2, 6],\n",
       "  },\n",
       "  (Assembly, self_loop, Assembly)={\n",
       "    edge_index=[2, 1],\n",
       "    edge_attr=[1, 7],\n",
       "  },\n",
       "  (Sink, self_loop, Sink)={\n",
       "    edge_index=[2, 1],\n",
       "    edge_attr=[1, 4],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f1abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GCNConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv,TransformerConv, HGTConv\n",
    "\n",
    "\n",
    "\n",
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in state.node_types:\n",
    "            in_channels = state.x_dict[node_type].shape[1]\n",
    "            self.lin_dict[node_type] = Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, state.metadata(),\n",
    "                           num_heads)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        return x_dict\n",
    "\n",
    "\n",
    "model = HGT(hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "\n",
    "# model = HGT(state.metadata())  # metadata = (node_types, edge_types)\n",
    "out = model(state.x_dict, state.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab606a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': tensor([[0., 1., 0., 1., 1., 0.],\n",
       "         [0., 0., 0., 1., 1., 0.]]),\n",
       " 'Assembly': tensor([[ 0.,  1.,  0.,  0.,  0.,  1., 20.]]),\n",
       " 'Sink': tensor([[0., 1., 0., 0.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871f047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['Source'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d9bf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_train.action_space.nvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f685945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ppo import Agent\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0931378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, input_feat_dim):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_feat_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_feat_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.action_space.nvec[0]), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceafef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define undefined variables (without using args)\n",
    "env_id = \"WaitingTime-v0\"\n",
    "exp_name = \"ppo_experiment\"\n",
    "seed = 42\n",
    "torch_deterministic = True\n",
    "cuda = True\n",
    "track = False\n",
    "wandb_project_name = \"ppo_project\"\n",
    "wandb_entity = None\n",
    "\n",
    "num_envs = 1\n",
    "num_steps = 128\n",
    "num_minibatches = 4\n",
    "total_timesteps = 100_000\n",
    "learning_rate = 2.5e-4\n",
    "anneal_lr = True\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "update_epochs = 4\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "clip_vloss = True\n",
    "norm_adv = True\n",
    "target_kl = None\n",
    "\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "num_iterations = total_timesteps // batch_size\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "if track:\n",
    "    import wandb\n",
    "    import types\n",
    "\n",
    "    wandb.init(\n",
    "            project=wandb_project_name,\n",
    "            entity=wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [make_env(env_id, i, capture_video, run_name) for i in range(num_envs)],\n",
    "# )\n",
    "line = WaitingTime(use_graph_as_states=True)\n",
    "envs = make_stacked_vec_env(\n",
    "    line=line,\n",
    "    simulation_end=100+1,\n",
    "    reward=\"parts\",\n",
    "    n_envs=1,\n",
    "    n_stack=1,\n",
    ")\n",
    "# assert isinstance(envs.action_space, gym.spaces.MultiDiscrete), \"only discrete action space is supported\"\n",
    "graph_encoder_feature_dim = 64\n",
    "graph_encoder = HGT(hidden_channels=graph_encoder_feature_dim, out_channels=4, num_heads=2, num_layers=1)\n",
    "agent = Agent(envs, graph_encoder_feature_dim).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "# Calculate the total encoded feature dimension\n",
    "# This should match the concatenated output from your graph encoder\n",
    "total_encoded_dim = 64 #* len(state.node_types)  # assuming each node type gets encoded to 64 dims\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs, total_encoded_dim)).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs), dtype=torch.long).to(device)  # Single action, not vector\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "\n",
    "# Move graph_encoder to device\n",
    "graph_encoder = graph_encoder.to(device)\n",
    "\n",
    "# Initial encoding of the hetero graph observation\n",
    "# Move HeteroData to device\n",
    "next_obs = next_obs.to(device)\n",
    "next_obs_graph = graph_encoder(next_obs.x_dict, next_obs.edge_index_dict)\n",
    "# Concatenate all node type embeddings into a single feature vector\n",
    "next_obs_encoded = next_obs_graph['Source'][1].unsqueeze(0)\n",
    "\n",
    "next_obs = next_obs_encoded  # This should be the encoded features, not raw HeteroData\n",
    "next_done = torch.zeros(num_envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65004025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73481ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HeteroData' has no attribute 'get_observations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m next_obs_raw, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m next_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n\u001b[1;32m     23\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/agile_stem/lineflow/lineflow/simulation/environment.py:194\u001b[0m, in \u001b[0;36mLineSimulation.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    191\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observations_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    197\u001b[0m     reward \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mget_n_parts_produced() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_parts) \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mscrap_factor\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mget_n_scrap_parts() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_scrap_parts)\n",
      "File \u001b[0;32m~/agile_stem/lineflow/lineflow/simulation/environment.py:256\u001b[0m, in \u001b[0;36mLineSimulation._get_observations_as_tensor\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_observations_as_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m--> 256\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observations\u001b[49m(lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, include_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/miniconda3/envs/agilestem/lib/python3.9/site-packages/torch_geometric/data/hetero_data.py:162\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dict$\u001b[39m\u001b[38;5;124m'\u001b[39m, key)):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'get_observations'"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, num_iterations + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if anneal_lr:\n",
    "        frac = 1.0 - (iteration - 1.0) / num_iterations\n",
    "        lrnow = frac * learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs_raw, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        \n",
    "        # Process the new HeteroData observation through graph encoder\n",
    "        # Move HeteroData to device\n",
    "        next_obs_raw = next_obs_raw.to(device)\n",
    "        next_obs_graph = graph_encoder(next_obs_raw.x_dict, next_obs_raw.edge_index_dict)\n",
    "        # Concatenate all node type embeddings into a single feature vector\n",
    "        next_obs = next_obs_graph['Source'][1].unsqueeze(0)\n",
    "    \n",
    "        next_done = torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1, total_encoded_dim))  # Use the correct encoded dimension\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)  # Single action dimension\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if target_kl is not None and approx_kl > target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382d4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agilestem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
